# The **stratosphere** project

**stratosphere** is a free and open source OSINT platform that automatically collects every page you visit, building a private knowledge base you can analyze with Jupyter notebooks and an extensible suite of web apps including:

* **Google sarch results**: Review your past Google search results
* **vk.com contacts explorer**: Explore previously seen vk.com contacts, highlighting their connections
* **Entity overview**: Navigat

You can also use the platform to quickly reverse-engineer HTTP/HTTPS web requests, including REST APIs. This lets you unlock the potential of private REST APIs, investigate GDPR breaches, and implement new scrapers to extend the knowledge base and power new web apps.

## Getting started

### 1. Install Docker

Docker is a tool that is used to automate the deployment of applications in lightweight containers so that
applications can work efficiently in different environments.
You can follow the [official installation instructions](https://docs.docker.com/get-docker/).

### 2. Pull the image and start the container

You can now download and run the image.
Execute from the command line:

```
docker pull micheda/stratosphere-app:latest
```

```
docker run --rm --name stratosphere-app -p 8080:8080 -p 127.0.0.1:8082:8082 \
  micheda/stratosphere-app:latest
```

After executing the last command, the container is running in foreground, 
with all services reporting a `successful` status.
If you want to interrupt the execution, simply press `ctrl-c` or `cmd-c`.

### 3. Configure your browser or device

You need to configure your browser or device to route all traffic through the HTTP proxy listening on:

```
http://localhost:8080
```

Browser versions and configurations frequently change, you should search the web on how to configure an HTTP proxy for your system.
Some operating systems have a global settings, some browsers have their own, other applications use environment variables, etc.

I recommend FoxyProxy, a browser extension that let you quickly switch between different proxy settings:

* [Firefox](https://addons.mozilla.org/it/firefox/addon/foxyproxy-standard/)
* [Chrome, Brave](https://chrome.google.com/webstore/detail/foxyproxy-standard/gcknhkkoolaabfmlnjonogaaifnjlfnp?hl=it)

#### Install the mitmproxy Certificate Authority

You can check that your web traffic is going through mitmproxy by browsing to http://mitm.it.
It should present you with a simple page to install the mitmproxy Certificate Authority, which is also the next step.
Follow the instructions for your OS / system and install the CA.

### 4. Test the web tracking

You can test that the system is working properly by browsing to https://www.google.com and verifying the presence of
a banner `[S]` on the top left corner of the page. Congratulations! **Stratosphere** is up and working properly.
The banner is always visible on all tracked pages. If missing, try refreshing the page with `ctrl-r`.
You can now access the dashboard by clicking on it or browsing to [http://localhost:8082](http://localhost:8082).

## Core concepts

### The knowledge base

The knowledge base is stored as an SQlite database (`/shared/data/kb.db`) with two tables: `entities` and `relationships`. An entity describes a "thing", such as a web search result or an user. A relationship describes the connenction between entities, such as a friendship between users.

#### Entities

Entities are uniquely identified by UUIDs that are generated by hashing their unique string identifiers. The definition of the unique string identifier is not fixed and it depends on the scraper that extracted it. For example, the entity of an user with ID `"123"` on social network `"xyz.com"` could be uniquely identified by the string `"xyz.com-123"`. Its UUID is defined by the 32-character hexadecimal string obtained from `UUID(hex=MD5("xyz.com-123"))`. The MD5 hashing algorithm, despite being cryptographically insecure, remains a sound choice in this context thanks to the length of its hashes (128 bits), that can be used with no further transformation as UUIDs. Each entity contains these attributes:

* `id`: UUID of the entity.
* `type`: Type of the entity, defined by extractor, always present. The current recommended format is `domain.name/typeofentity`. For example, `"google.com/search_query"` represents search results.
* `data`: JSON dictionary of properties. The schema depends on the type of entity and the available extracted data. If no data is available, the field is set to NULL.
* `blob`: Binary field, currently not used. It might be used to contain binary data such as images.
* `ts`: Timestamp of the captured flow (`flow.response.timestamp_start`).

The knowledge base contains always a NIL entity whose `id` column is seto to zeros (`"000..."`). NIL entities (and relationships) help simplify the code.

#### Relationships

Relationships are uniquely identified by the UUIDs obtained from hashing the concatenation of their UUIDs. Relationships might be directional. For example, the relationship between entities with UUIDs `"1.."` and `"2.."` is defined as `UUID(hex=MD5("1..2.."))`. Each entity contains these attributes:

* `id`: UUID of the relationship.
* `type`: Defined as in entities but applied to relationships.
* `src`: Source UUID of the relationship, always present.
* `dst`: Destination UUID of the relationship, always present.
* `data`: Defined as in entities but applied to relationships.
* `blob`: Defined as in entities but applied to relationships.
* `ts`: Defined as in entities but applied to relationships.

Similarly to the entities table, a NIL relationship is also always present, with  `id`, `src`, and `dst` columns are set to zeros.

#### The importance of stable UUIDs to accumulate knowledge

The duty of the extractors is to process flows to insert new entities and relationships in the knowledge base.
The properties in the `data` columns might differ over time, depending on the different interactions with websites.
For example, the field `"birthdate"` might be available only in the user profile page and the field `"full_name"` might be  available in a friends page. It is important to define a stable UUID for the same entity to enable the consolidastion of multiple entities describing the same "thing" into a single merged entity.

### Adding new Jupyter notebooks and Voilà web apps

You can access JupyterLab from the main dashboard. The notebooks located in the subdirectory `webapps` are also published as Voilà web applications.

The example notebook `01 kb overview.ipynb` shows how to query the knowledge base with SQL and Pandas. The `stratosphere` Python package is included directly from source extending the `PYTHONPATH` env variable and it is located at `/shared/src/stratosphere`. Modifications to the source code are hot-reloaded in the notebooks thanks to `%autoreload` (useful during development).

### The system architecture

The system relies on [mitmproxy](https://mitmproxy.org/) to intercept the web traffic (both desktop and mobile), building a knowledge base with [SQLite](https://sqlite.org/) that is later accessed by a suite of web apps built with [Jupyter](https://jupyter.org/) and [Voilà](https://voila.readthedocs.io/en/stable/). [supervisor])http://supervisord.org/) is used to manage the running services. The architecture is cross platform and runs locally inside a Docker container. The system includes these running services (entry points in `/shared/services/`):

* **mitmproxy**: running the HTTP/S proxy and dumping the flows to `probe.db`.
* **extractor**: reading the flows from `probe.db`, adding entities and relationships to `kb.db`. The pipeline is retriggered every `10` seconds and it will delete all flows older than `10` minutes, possibly reprocessing already seen flows. This procedure ensures that recent traffic can always be inspected in `probe.db` without retaining the whole flows history.
* **nginx**: proxying all services behind http://localhost:8082.
  * **jupterlab/Voilà**: JupyterLab server with Voilà extension to serve the web apps.
  * **sqliteweb**: Web-based SQLite database browser pointing to `kb.db`.

### How to add a new scraper

Scrapers are executed by the **extractor** service.

* Flows (raw web requests and responses) are dumped in the `flows` table in the SQLite database `probe.db`. The flows are regularly processed by the scrapers before being removed, ensuring that the file size remains under control. The columns in this table map to the attributes in the `Flow` objects in mitmproxy ([official documentation](https://docs.mitmproxy.org/stable/api/mitmproxy/flow.html)). For example, the contents and meaning of field `flow_response_content` is documented [here](https://docs.mitmproxy.org/stable/api/mitmproxy/http.html#Response). The additional column `id` is a random UUID.

* `02 capture sample.ipynb` lets you capture a sample of flows for later analysis.
* `03 analyze sample.ipynb` helps you analyze the contents of a captured sample of flows.
* `04 test extractors.ipynb` tests the extractors on the captured flow samples.

## Development

### How does it work?


### Useful Docker parameters

Some useful options if you want to customise it:

* [--rm](https://docs.docker.com/engine/reference/run/#clean-up---rm): Docker will automatically clean up the container and remove the file system when the container exits. If you want to retain the container’s file system, remove `--rm`.
* [-p](https://docs.docker.com/engine/reference/run/#expose-incoming-ports): Publish the container's port on the host. The format is `host_ip:host_port:container_port`. If you drop the `host_ip`, the port will be published on all interfaces. By default, the proxy runs on all intefaces, but the web interface is accessible only from localhost.
* [--name](https://docs.docker.com/engine/reference/run/#name---name): Name your container. 
* [-d](https://docs.docker.com/engine/reference/run/#detached-vs-foreground): Start the container in background.
* [-it](https://docs.docker.com/engine/reference/run/#foreground): Allocate a pseudo-tty and keep STDIN open even if not attached. Useful if you want to access directly the container from terminal, via `docker execute`.
* [-v](https://docs.docker.com/engine/reference/run/#volume-shared-filesystems): Bind mount a volume. For development and persistance, you can mount the absolute path of the `stratosphere-app` directory to `/shared` with `-v /absolute-path-to-stratosphere-repos/stratosphere-app/:/shared`. On Windows, the host path (before `:`) must be a valid Windows path.